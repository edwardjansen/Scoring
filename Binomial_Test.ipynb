{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import display libs\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edward\\AppData\\Roaming\\Python\\Python36\\site-packages\\matplotlib\\__init__.py:846: MatplotlibDeprecationWarning: \n",
      "The text.latex.unicode rcparam was deprecated in Matplotlib 2.2 and will be removed in 3.1.\n",
      "  \"2.2\", name=key, obj_type=\"rcparam\", addendum=addendum)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Edward\\\\.matplotlib'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# setup figure params\n",
    "figureparams = {'axes.labelsize': 24,\n",
    "           'axes.titlesize': 20,\n",
    "           'axes.linewidth': 1.3,\n",
    "           'font.size': 20,\n",
    "           'legend.fontsize': 18,\n",
    "           'figure.figsize': (10,7),\n",
    "           'font.family': 'serif',\n",
    "           'font.serif': 'Computer Modern Roman',\n",
    "           'xtick.labelsize': 18,\n",
    "           'xtick.major.size': 5.5,\n",
    "           'xtick.major.width': 1.3,\n",
    "           'ytick.labelsize': 18,\n",
    "           'ytick.major.size': 5.5,\n",
    "           'ytick.major.width': 1.3,\n",
    "           'text.usetex': True,\n",
    "           'figure.autolayout': True}\n",
    "plt.rcParams.update(figureparams)\n",
    "matplotlib.rcParams['text.usetex']=True\n",
    "matplotlib.rcParams['text.latex.unicode']=True\n",
    "matplotlib.get_configdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binomial test\n",
    "### Contents\n",
    "    1. Binomial test\n",
    "        a. Right-sided test\n",
    "        b. Left-sided test\n",
    "        c. Two-sided test\n",
    "    2. Confidence Intervals\n",
    "        a. Clopper-Pearson\n",
    "        b. Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Binomial test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binomial test is arguably the simplest of all statistical tests. Let's say we flip a coin $N$ times and we observe $k$ successes. Let's say we expected a probability $p \\neq k/ N$. Of course, because of the variance of this process, we would not expect to observe exactly $pN$ successes. However, this begs the question: when can we say that our observed probability is statistically different from the expected probability?\n",
    "\n",
    "We need to distinguish three cases. In all cases, our null hypothesis will be that the our expected probability is the real probability: \n",
    "- $H_0$: $p_\\text{real} = p_\\text{exp}$.\n",
    "\n",
    "The difference is in the alternative hypotheses. We can have:\n",
    "- $H_A$: $p_\\text{real} \\geq p_\\text{exp}$;\n",
    "- $H_A$: $p_\\text{real} \\leq  p_\\text{exp}$;\n",
    "- $H_A$: $p_\\text{real} \\neq p_\\text{exp}$.\n",
    "\n",
    "The first two are called one-sided tests, whereas the second is called a two-sided test. Often, the number of observations will hint at whether one wants to conduct a left- or right-sided test. The two-sided test is obviously the most general of the three. Let's now implement them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a. Right-sided binomial test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the zero hypothesis, we would like to compute the probability that we observe $k$ or more successes in $N$ trials assuming the generating probability to be $p_\\text{expected}$:\n",
    "\\begin{equation}\n",
    "P\\left( X \\geq k \\hspace{2pt} | \\hspace{2pt} H_0   \\right) = \\sum_{i = k}^N {n \\choose k} p_\\text{exp}^k (1-p_\\text{exp})^{n-k}.\n",
    "\\end{equation}\n",
    "\n",
    "If this probability - often referred to as the p-value - is smaller than some preset significance level $\\alpha$, we can reject the zero hypothesis in favor of the alternative hypothesis. Typical values for $\\alpha$ are $0.05$ and $0.01$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set basic parameters\n",
    "N = 1000        # number of obs\n",
    "p = .32         # expected default probability\n",
    "\n",
    "# generate data with underlying probability p_real\n",
    "p_real = .40\n",
    "rand_data = np.random.rand(N)\n",
    "idx = np.where(rand_data < p_real)[0]\n",
    "obs = np.zeros(N)                                 \n",
    "obs[idx] = 1 \n",
    "k = np.sum(obs).astype(int) # number of defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our p-value: 6.702613548517755e-07\n",
      "Scipys p-value: 6.702613548516461e-07\n"
     ]
    }
   ],
   "source": [
    "# One-sided Binomial test (greater)\n",
    "def binom_test_greater(N, k, p):\n",
    "    probs = []\n",
    "    for i in range(k, N+1):\n",
    "        fact = np.math.factorial(N) / ( np.math.factorial(i) * np.math.factorial(N-i) )\n",
    "        tmp = fact * p**i * (1-p)**(N-i)\n",
    "        probs.append(tmp)\n",
    "    \n",
    "    return np.sum(probs)\n",
    "        \n",
    "p_val = binom_test_greater(N, k, p)\n",
    "\n",
    "\n",
    "# let's check with statsmodels\n",
    "from statsmodels.stats.proportion import binom_test\n",
    "p_val_scipy = binom_test(k, N, p, alternative='larger')\n",
    "\n",
    "print(\"Our p-value: \" + str(p_val))\n",
    "print(\"Scipys p-value: \" + str(p_val_scipy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. Left-sided binomial test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, to test the zero hypothesis, we would like to compute the probability that we observe $k$ or less successes in $N$ trials assuming the generating probability to be $p_\\text{expected}$:\n",
    "\\begin{equation}\n",
    "P\\left( X \\leq k \\hspace{2pt} | \\hspace{2pt} H_0   \\right) = \\sum_{i = 0}^k {n \\choose k} p_\\text{exp}^k (1-p_\\text{exp})^{n-k}.\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set basic parameters\n",
    "N = 1000        # number of obs\n",
    "p = .32         # expected default probability\n",
    "\n",
    "# generate data with underlying probability p_real\n",
    "p_real = .20\n",
    "rand_data = np.random.rand(N)\n",
    "idx = np.where(rand_data < p_real)[0]\n",
    "obs = np.zeros(N)                                 \n",
    "obs[idx] = 1 \n",
    "k = np.sum(obs).astype(int) # number of defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our p-value: 2.056012793832713e-15\n",
      "Scipys p-value: 2.0560127938321348e-15\n"
     ]
    }
   ],
   "source": [
    "# One-sided Binomial test (smaller)\n",
    "def binom_test_smaller(N, k, p):\n",
    "    probs = []\n",
    "    for i in range(0, k+1):\n",
    "        fact = np.math.factorial(N) / ( np.math.factorial(i) * np.math.factorial(N-i) )\n",
    "        tmp = fact * p**i * (1-p)**(N-i)\n",
    "        probs.append(tmp)\n",
    "    \n",
    "    return np.sum(probs)\n",
    "        \n",
    "p_val = binom_test_smaller(N, k, p)\n",
    "\n",
    "\n",
    "# let's check with statsmodels\n",
    "from statsmodels.stats.proportion import binom_test\n",
    "p_val_scipy = binom_test(k, N, p, alternative='smaller')\n",
    "\n",
    "print(\"Our p-value: \" + str(p_val))\n",
    "print(\"Scipys p-value: \" + str(p_val_scipy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1c. Two-sided binomial test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two-sided test tests whether at a given deviation $\\delta = |k| - Np_\\text{exp}$ the number of successes $k$ is too high or too low. The p-value is computed as\n",
    "\\begin{equation}\n",
    "P\\left( X \\neq k \\hspace{2pt} | \\hspace{2pt} H_0   \\right) = \\sum_{i = 0}^{Np_\\text{exp} - \\delta} {n \\choose k} p_\\text{exp}^k (1-p_\\text{exp})^{n-k} + \\sum_{i = Np_\\text{exp} + \\delta}^{N} {n \\choose k} p_\\text{exp}^k (1-p_\\text{exp})^{n-k}.\n",
    "\\end{equation}\n",
    "\n",
    "Note that, for symmetrical distributions, the two-sided test will be equal to twice the left- or right-sided test.\n",
    "However, the binomial distribution is not symmetrical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our p-value: 8.327587638381893e-14\n",
      "Scipys p-value: 3.5117235929402072e-15\n"
     ]
    }
   ],
   "source": [
    "# compute two-sided test: use same data as before\n",
    "def binom_test_two_sided(N, k, p):\n",
    "    probs = []\n",
    "    dev = np.abs(k - N*p) \n",
    "    left_tail = np.round( N*p - dev ).astype(int)\n",
    "    right_tail = np.round( N*p + dev ).astype(int)\n",
    "    \n",
    "    for i in range(0, left_tail+1):\n",
    "        fact = np.math.factorial(N) / ( np.math.factorial(i) * np.math.factorial(N-i) )\n",
    "        tmp = fact * p**i * (1-p)**(N-i)\n",
    "        probs.append(tmp)        \n",
    "    for i in range(right_tail, N+1):\n",
    "        fact = np.math.factorial(N) / ( np.math.factorial(i) * np.math.factorial(N-i) )\n",
    "        tmp = fact * p**i * (1-p)**(N-i)\n",
    "        probs.append(tmp)           \n",
    "    return np.sum(probs)\n",
    "        \n",
    "p_val = binom_test_two_sided(N, k, p)\n",
    "\n",
    "\n",
    "# let's check with statsmodels\n",
    "from statsmodels.stats.proportion import binom_test\n",
    "p_val_scipy = binom_test(k, N, p, alternative='two-sided')\n",
    "\n",
    "print(\"Our p-value: \" + str(p_val))\n",
    "print(\"Scipys p-value: \" + str(p_val_scipy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Confidence intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we used the p-value approach to say something about the statistical significance of a certain statistic (in this case the observed number of successes). Although often used (e.g. in science), some argue that it is better practice to report confidence intervals instead of significance levels. The reason would be that confidence intervals provide more information than significance levels. \n",
    "\n",
    "Often, there are two ways to go about setting up confidence intervals. The first is to find an analytical expression for the confidence interval. This is for example the case for the Clopper-Pearson confidence interval below. Another way is the bootstrap approach, in which one uses the bootstrap principle to recompute the test-statistic (in this case the number of successes) $N_\\text{boots}$ times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a. Clopper-Pearson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Clopper-Pearson interval is given as $\\left(\\frac{k}{N} - \\delta_1, \\frac{k}{N} + \\delta_2 \\right)$ with confidence level $1- \\alpha$, where $\\delta_i$ is the largest value for which with significance $\\alpha /2$ the following hypotheses tests succeed:\n",
    "- $H_0$: $k_\\text{real} = \\frac{k}{n} - \\delta_1$ and $H_A$: $k_\\text{real} > \\frac{k}{n} - \\delta_1$;\n",
    "- $H_0$: $k_\\text{real} = \\frac{k}{n} + \\delta_2$ and $H_A$: $k_\\text{real} < \\frac{k}{n} - \\delta_1$;\n",
    "\n",
    "One can show that - because of the relation between the binomial distriubtion and the $\\beta$-distribution - the CP-interval is equivalently given in terms of the $\\beta$-distribution as\n",
    "\\begin{equation}\n",
    "\\left( B \\left( \\frac{\\alpha}{2}, k, N-k-1 \\right) , B \\left( 1 - \\frac{\\alpha}{2}, k+1, N-k \\right) \\right),\n",
    "\\end{equation}\n",
    "which allows for easy implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with 95% conf., real p lies within: (0.17, 0.28)\n",
      "observed p: 0.22\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import beta\n",
    "\n",
    "alpha = 0.05\n",
    "CI_low = beta.ppf(alpha/2, k, N-k+1)\n",
    "CI_up = beta.ppf(1-alpha/2, k+1, N-k)\n",
    "print(\"with 95% conf., real p lies within: (\" + str(np.round(CI_low, 2)) + \", \" + str(np.round(CI_up,2)) + \")\")\n",
    "print(\"observed p: \"+ str(np.round(k/N,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b. Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually, it is easier to use bootstrapping. The idea is simple: given our sample, we compute our test statistic and then create a new sample by resampling from the sample with replacement. Then, we can recompute the test-statistic, which will be somewhat different. This process is the repeated $N_\\text{boot}$ times. Finally, we can take the $\\frac{\\alpha}{2}$th and $1 - \\frac{\\alpha}{2}$th percentile of the resulting distribution of test statistics as confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "p = .32\n",
    "rand_data = np.random.rand(N)\n",
    "idx = np.where(rand_data < p)[0]\n",
    "obs = np.zeros(N)                                 \n",
    "obs[idx] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's check our expected probability\n",
    "k = np.sum(obs)\n",
    "p_expected = k / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with 95% conf., real p lies within: (0.32, 0.38)\n",
      "observed p: 0.35\n"
     ]
    }
   ],
   "source": [
    "# let's use Clopper-Pearson to create a reference\n",
    "alpha = 0.05\n",
    "CI_low = beta.ppf(alpha/2, k, N-k+1)\n",
    "CI_up = beta.ppf(1-alpha/2, k+1, N-k)\n",
    "print(\"with 95% conf., real p lies within: (\" + str(np.round(CI_low, 2)) + \", \" + str(np.round(CI_up,2)) + \")\")\n",
    "print(\"observed p: \"+ str(np.round(k/N,2)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's now bootstrap the CI\n",
    "def bootstrap(data, n=1000, func=np.sum):\n",
    "    \"\"\"\n",
    "    Generate `n` bootstrap samples, evaluating `func`\n",
    "    at each resampling. `bootstrap` returns a function,\n",
    "    which can be called to obtain confidence intervals\n",
    "    of interest.\n",
    "    \"\"\"\n",
    "    simulations = list()\n",
    "    sample_size = len(data)\n",
    "    for i in range(n):\n",
    "        itersample = np.random.choice(data, size=sample_size, replace=True)\n",
    "        simulations.append(func(itersample) / sample_size)\n",
    "    simulations.sort()\n",
    "    return simulations\n",
    "\n",
    "def confidence_interval(sim, alpha):\n",
    "    \"\"\"\n",
    "    Return 2-sided symmetric confidence interval specified\n",
    "    by p.\n",
    "    \"\"\"\n",
    "    sim.sort()\n",
    "    n = len(sim)\n",
    "    u_pval = 1 - alpha/2\n",
    "    l_pval = alpha / 2\n",
    "    l_idx = int(np.floor(n*l_pval))\n",
    "    u_idx = int(np.floor(n*u_pval))\n",
    "    return(sim[l_idx], sim[u_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap: real p lies within: (0.319, 0.379)\n",
      "Clopper-Pearson:, real p lies within: (0.32, 0.38)\n",
      "Observed p: 0.35\n"
     ]
    }
   ],
   "source": [
    "# run the bootstrap\n",
    "sim = bootstrap(obs)\n",
    "bounds = confidence_interval(sim, alpha = 0.05)\n",
    "\n",
    "print(\"Bootstrap: real p lies within: (\" + str(bounds[0]) + \", \" + str(bounds[1]) + \")\")\n",
    "print(\"Clopper-Pearson:, real p lies within: (\" + str(np.round(CI_low, 2)) + \", \" + str(np.round(CI_up,2)) + \")\")\n",
    "print(\"Observed p: \"+ str(np.round(k/N,2)))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
